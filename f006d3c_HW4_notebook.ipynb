{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGn3arFx-Eze"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/drive/MyDrive/Project_4_export.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90JH3fKuY5Sc",
        "outputId": "73d4eba0-ab34-40af-9b05-772a580ac2ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Long-Short Term Memory (LSTM) for Sentiment Analysis (20 points)\n",
        "Your task is to create an LSTM network to predict sentiment from Yelp reviews (train_yelp_reviews.csv).\n",
        "A review is considered positive if labeled with a `1` and negative if labeled with a `0`. (Hint: Read\n",
        "the provided CSV files with pandas using sep =`\\t` and engine=`python`)\n",
        "\n",
        "You may use any packages or tools as you wish, however, the code that you submit must be\n",
        "written on your own. You are free to experiment with pre-processing, model architectures, training\n",
        "procedures, removing stop-words, or hyper-parameters, as long as your model contains at least one\n",
        "LSTM layer. You may find using a GPU to be beneficial, but we have made sure that good classification\n",
        "can be obtained using an average CPU."
      ],
      "metadata": {
        "id": "xlzL5VH2-5o8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Data Inspection (5 points)\n",
        "Read through some of the reviews. Display the most 50 commonly occurring tokens for each class.\n",
        "Are there any patterns in reviews that Naive Bayes or other bag-of-word models may not be able\n",
        "to classify accurately?"
      ],
      "metadata": {
        "id": "2L2cOyQO_Um1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import re\n",
        "warnings.filterwarnings('ignore') \n",
        "from numpy.random import seed\n",
        "seed(10)\n",
        "\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "9gUJosSe-2Eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('/content/datasets/train_yelp_reviews.csv', sep='\\t', engine = 'python')\n",
        "train_data.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "S8WOQ6wf_daX",
        "outputId": "757b4408-e925-41be-f976-735470ea4a94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  label\n",
              "0      Great time - family dinner on a Sunday night.      1\n",
              "1      The classic Maine Lobster Roll was fantastic.      1\n",
              "2                            We won't be going back.      0\n",
              "3       All I have to say is the food was amazing!!!      1\n",
              "4  Food was good, service was good, Prices were g...      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9750388c-e7eb-453d-8cb6-233be0fc7f98\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Great time - family dinner on a Sunday night.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The classic Maine Lobster Roll was fantastic.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>We won't be going back.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>All I have to say is the food was amazing!!!</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Food was good, service was good, Prices were g...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9750388c-e7eb-453d-8cb6-233be0fc7f98')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9750388c-e7eb-453d-8cb6-233be0fc7f98 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9750388c-e7eb-453d-8cb6-233be0fc7f98');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#positive reviews and negative reviews\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "from nltk.probability import FreqDist\n",
        "import re\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "punctuations = set(string.punctuation)\n",
        "\n",
        "positive_reviews = []\n",
        "negative_reviews = []\n",
        "\n",
        "## implementation understanding taken from chatgpt\n",
        "for idx, row in train_data.iterrows():\n",
        "  tokens = word_tokenize(row['text'])  \n",
        "  tokens = [token.lower() for token in tokens if token.lower() not in stop_words and token not in punctuations]\n",
        "\n",
        "\n",
        "  if row['label'] == 1:\n",
        "    positive_reviews.extend(tokens)\n",
        "\n",
        "  if row['label'] == 0:\n",
        "    negative_reviews.extend(tokens)\n",
        "\n",
        "## referenced from https://tedboy.github.io/nlps/generated/generated/nltk.FreqDist.html\n",
        "positive_reviews_dict = FreqDist(positive_reviews)\n",
        "negative_reviews_dict = FreqDist(negative_reviews)\n",
        "\n",
        "positive_reviews_dict.most_common(50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LHQgf9cAz1u",
        "outputId": "11f4500d-b249-4c80-e5e5-40e7f764acbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('good', 66),\n",
              " ('great', 64),\n",
              " ('food', 53),\n",
              " ('place', 51),\n",
              " ('service', 42),\n",
              " ('friendly', 22),\n",
              " ('delicious', 21),\n",
              " ('amazing', 20),\n",
              " ('back', 20),\n",
              " ('really', 20),\n",
              " ('best', 20),\n",
              " ('nice', 19),\n",
              " ('time', 18),\n",
              " (\"'s\", 18),\n",
              " (\"n't\", 17),\n",
              " ('go', 17),\n",
              " ('also', 16),\n",
              " ('like', 16),\n",
              " ('restaurant', 15),\n",
              " ('staff', 13),\n",
              " ('...', 13),\n",
              " ('fantastic', 12),\n",
              " ('always', 12),\n",
              " ('vegas', 12),\n",
              " ('love', 12),\n",
              " ('awesome', 12),\n",
              " ('pretty', 11),\n",
              " ('menu', 11),\n",
              " ('first', 10),\n",
              " ('fresh', 10),\n",
              " ('excellent', 10),\n",
              " ('steak', 10),\n",
              " ('pizza', 10),\n",
              " ('experience', 10),\n",
              " ('even', 10),\n",
              " ('perfect', 9),\n",
              " ('chicken', 9),\n",
              " ('atmosphere', 9),\n",
              " ('server', 9),\n",
              " ('one', 9),\n",
              " ('prices', 8),\n",
              " ('stars', 8),\n",
              " ('everything', 8),\n",
              " ('made', 8),\n",
              " ('spot', 8),\n",
              " ('loved', 8),\n",
              " ('definitely', 8),\n",
              " ('ever', 8),\n",
              " ('well', 8),\n",
              " ('come', 8)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "negative_reviews_dict.most_common(50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KpSC85mDTgr",
        "outputId": "47f16a1b-d1df-46d8-a8c9-ecba6c4a972a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"n't\", 68),\n",
              " ('food', 60),\n",
              " ('place', 40),\n",
              " ('back', 33),\n",
              " ('service', 31),\n",
              " ('like', 28),\n",
              " ('go', 25),\n",
              " ('would', 20),\n",
              " ('good', 20),\n",
              " ('time', 19),\n",
              " (\"'s\", 18),\n",
              " ('never', 17),\n",
              " ('bad', 16),\n",
              " ('minutes', 16),\n",
              " ('ever', 16),\n",
              " ('one', 15),\n",
              " ('...', 14),\n",
              " (\"'ve\", 14),\n",
              " ('disappointed', 14),\n",
              " ('got', 13),\n",
              " ('much', 12),\n",
              " ('think', 12),\n",
              " ('us', 12),\n",
              " ('really', 12),\n",
              " (\"'m\", 12),\n",
              " ('came', 12),\n",
              " ('wo', 11),\n",
              " ('going', 11),\n",
              " ('worst', 11),\n",
              " ('get', 10),\n",
              " ('better', 10),\n",
              " ('eat', 10),\n",
              " ('probably', 9),\n",
              " ('even', 9),\n",
              " ('``', 9),\n",
              " (\"''\", 9),\n",
              " ('bland', 9),\n",
              " ('slow', 9),\n",
              " ('wait', 9),\n",
              " ('terrible', 9),\n",
              " ('waited', 9),\n",
              " ('also', 8),\n",
              " ('burger', 8),\n",
              " ('way', 8),\n",
              " ('flavor', 8),\n",
              " ('restaurant', 8),\n",
              " ('experience', 8),\n",
              " ('coming', 8),\n",
              " ('ordered', 8),\n",
              " ('another', 8)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The patterns in reviews that Naive Bayes or other bag-of-word models can find difficult to categorize correctly are frequently connected to the meaning of words in context. Bag-of-word models do not take into account the order or sequence of words in a sentence and instead treat each word as an independent property. "
      ],
      "metadata": {
        "id": "uOxD7XqYaffS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Model Training (15 points)\n",
        "Implement your LSTM model and apply it to the training dataset. In order to obtain full credit, you\n",
        "must describe your text pre-processing procedure, describe your network in terms of the layers used,\n",
        "input/output dimensions at each layer, choice of word embedding dictionary, and training procedure."
      ],
      "metadata": {
        "id": "MBjOvOF1EPY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_data['text'])\n",
        "sequences = tokenizer.texts_to_sequences(train_data['text'])\n",
        "\n",
        "# Pad sequences to ensure equal length\n",
        "max_len_seq = 0\n",
        "for seq in sequences:\n",
        "  len_seq = len(seq)\n",
        "  if max_len_seq < len_seq:\n",
        "    max_len_seq = len_seq\n",
        "    \n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len_seq)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "xtrain, xval, ytrain, yval = train_test_split(padded_sequences, train_data['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "xtrain.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pdriRR_Ddy7",
        "outputId": "a7558465-f29f-4cac-8659-e2771f9b6e63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(720, 32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##LSTM\n",
        "model = Sequential()\n",
        "model.add(tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=200, input_length=max_len_seq))\n",
        "model.add(tf.keras.layers.SpatialDropout1D(0.4))\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "# model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.SpatialDropout1D(0.3))\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\n",
        "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=2, cooldown=3, mode = 'min')\n",
        "early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
        "loss = tf.keras.losses.BinaryCrossentropy( \n",
        "    from_logits = True\n",
        ")\n",
        "\n",
        "metric = tf.keras.metrics.BinaryAccuracy(\n",
        "    name='accuracy',\n",
        ")\n",
        "\n",
        "model.compile(loss=loss,\n",
        "              optimizer='adam',\n",
        "              metrics=[metric])"
      ],
      "metadata": {
        "id": "N7uIfOjwRF2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import *\n",
        "\n",
        "num_epochs = 10\n",
        "history = model.fit(xtrain, ytrain, \n",
        "                    epochs=num_epochs, verbose=1,\n",
        "                    validation_data = (xval, yval),\n",
        "                    batch_size = 32,\n",
        "                    callbacks = [reduce_lr, early])\n",
        "\n",
        "prediction = model.predict(xval)\n",
        "\n",
        "# Get labels based on probability 1 if p>= 0.5 else 0\n",
        "pred_labels = []\n",
        "for i in prediction:\n",
        "    if i >= 0.5:\n",
        "        pred_labels.append(1)\n",
        "    else:\n",
        "        pred_labels.append(0)\n",
        "print(\"Report of prediction on test set : \\n \", classification_report(yval, pred_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65mYdsJ1RkRW",
        "outputId": "08f4d54a-dcef-4738-87fb-3aab64b653ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "23/23 [==============================] - 19s 239ms/step - loss: 0.6956 - accuracy: 0.5347 - val_loss: 0.6922 - val_accuracy: 0.5056 - lr: 0.0010\n",
            "Epoch 2/10\n",
            "23/23 [==============================] - 3s 133ms/step - loss: 0.6820 - accuracy: 0.5542 - val_loss: 0.6925 - val_accuracy: 0.5056 - lr: 0.0010\n",
            "Epoch 3/10\n",
            "23/23 [==============================] - 4s 168ms/step - loss: 0.6014 - accuracy: 0.6861 - val_loss: 0.6811 - val_accuracy: 0.5167 - lr: 0.0010\n",
            "Epoch 4/10\n",
            "23/23 [==============================] - 3s 122ms/step - loss: 0.3343 - accuracy: 0.8681 - val_loss: 0.6244 - val_accuracy: 0.7722 - lr: 0.0010\n",
            "Epoch 5/10\n",
            "23/23 [==============================] - 2s 72ms/step - loss: 0.1543 - accuracy: 0.9403 - val_loss: 0.5825 - val_accuracy: 0.7389 - lr: 0.0010\n",
            "Epoch 6/10\n",
            "23/23 [==============================] - 2s 78ms/step - loss: 0.1163 - accuracy: 0.9611 - val_loss: 0.5451 - val_accuracy: 0.7167 - lr: 0.0010\n",
            "Epoch 7/10\n",
            "23/23 [==============================] - 1s 38ms/step - loss: 0.0495 - accuracy: 0.9847 - val_loss: 0.5231 - val_accuracy: 0.7611 - lr: 0.0010\n",
            "Epoch 8/10\n",
            "23/23 [==============================] - 1s 51ms/step - loss: 0.0309 - accuracy: 0.9917 - val_loss: 0.4992 - val_accuracy: 0.7444 - lr: 0.0010\n",
            "Epoch 9/10\n",
            "23/23 [==============================] - 2s 72ms/step - loss: 0.0220 - accuracy: 0.9931 - val_loss: 0.5694 - val_accuracy: 0.7278 - lr: 0.0010\n",
            "Epoch 10/10\n",
            "23/23 [==============================] - 1s 60ms/step - loss: 0.0169 - accuracy: 0.9944 - val_loss: 0.6446 - val_accuracy: 0.7333 - lr: 0.0010\n",
            "6/6 [==============================] - 1s 6ms/step\n",
            "Report of prediction on test set : \n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.60      0.69        89\n",
            "           1       0.69      0.87      0.77        91\n",
            "\n",
            "    accuracy                           0.73       180\n",
            "   macro avg       0.75      0.73      0.73       180\n",
            "weighted avg       0.75      0.73      0.73       180\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"F1 Score : {}\".format(f1_score(yval, pred_labels)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9HDEOq7aSr1",
        "outputId": "76fca375-de63-46a7-a723-dea68ced17a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score : 0.7669902912621359\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Confusion Matrix : \\n {}\".format(confusion_matrix(yval, pred_labels)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKz5gf9w3-6j",
        "outputId": "b3b6aac9-2987-4a2f-a63f-f2b230155b3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix : \n",
            " [[53 36]\n",
            " [12 79]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Model Prediction\n",
        "Apply your trained model to the test data (test_yelp_reviews.csv). Save your predictions as a\n",
        "new file in single column titled \"prediction\" and export as a CSV file. Please submit your saved\n",
        "CSV to Canvas with the name (<YourNetID>_LSTM_predictions.csv). Your grade on the model\n",
        "will be based on the accuracy of your predictions on the unlabeled data. Below is an example\n",
        "output:"
      ],
      "metadata": {
        "id": "pklxQMGKZNK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "test_data = pd.read_csv('/content/datasets/test_yelp_reviews.csv', sep='\\t',engine='python')\n",
        "\n",
        "# test_data['text'] = test_data['text'].apply(lambda x: [token.lower() for token in tokens if token.lower() not in stop_words and token not in punctuations])\n",
        "# test_data['text'] = test_data['text'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# # Convert the text data to sequences or embeddings if required by your model\n",
        "test_sequences = tokenizer.texts_to_sequences(test_data['text'])\n",
        "\n",
        "# # Pad sequences to a fixed length\n",
        "test_sequences = pad_sequences(test_sequences, maxlen=max_len_seq)\n",
        "\n",
        "# Make predictions\n",
        "test_preds = model.predict(test_sequences)\n",
        "test_preds = test_preds.flatten()\n",
        "\n",
        "pred_labels = []\n",
        "for i in test_preds:\n",
        "    if i >= 0.5:\n",
        "        pred_labels.append(1)\n",
        "    else:\n",
        "        pred_labels.append(0)\n",
        "# Create a DataFrame with predictions\n",
        "preds_df = pd.DataFrame({'predictions': pred_labels})\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "preds_df.to_csv('/content/drive/MyDrive/f006d3c_LSTM_predictions.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wl4uzZ05XnOS",
        "outputId": "846059f0-a49e-4012-b07d-58059d542e24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 8ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. FineTune Bert Model (20 points)\n",
        "Using the same dataset, fine-tune a pre-trained BERT model for sentiment analysis. You are\n",
        "recommended to use the Hugging Face implementation of BERT model and pre-trained weights."
      ],
      "metadata": {
        "id": "mk8GpRNueReU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "• Preprocess the dataset into a format that can be used by BERT model."
      ],
      "metadata": {
        "id": "W2YoMf1q8dED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertForSequenceClassification, BertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from numpy.random import seed\n",
        "seed(100)\n",
        "\n",
        "xtrain, xtest = train_test_split(train_data, test_size=0.2, random_state=45)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the text and encode labels\n",
        "train_encodings = tokenizer(list(xtrain['text']), truncation=True, padding=True)\n",
        "val_encodings = tokenizer(list(xtest['text']), truncation=True, padding=True)\n",
        "\n",
        "# Create TensorFlow dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    xtrain['label'].values\n",
        "))\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    xtest['label'].values\n",
        "))"
      ],
      "metadata": {
        "id": "nQE3Pam6Xevx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tune the pre-trained BERT model using the training set and evaluate its performance on\n",
        "the validation dataset."
      ],
      "metadata": {
        "id": "_V9iWhy48e1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained BERT model\n",
        "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define the optimizer, loss function, and metrics optimizer, and loss suggested by chatgpt\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "#asked chatgpt about recommended metrics for BERT, and it suggested this\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "# callbacks = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=5)\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=2, cooldown=3, mode = 'min')\n",
        "early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "# Compile the model\n",
        "bert_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "#bert_model.compile(optimizer=optimizer, loss=loss, metrics='accuracy')\n",
        "\n",
        "# Train the model\n",
        "bert_model.fit(\n",
        "    train_dataset.shuffle(1000).batch(64),\n",
        "    epochs=15,\n",
        "    batch_size=32,\n",
        "    validation_data=val_dataset.shuffle(1000).batch(64),\n",
        "    callbacks = [reduce_lr, early]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FynkpM2cbA4d",
        "outputId": "1ed8ab79-1cbc-48f2-e410-1d30f12fb483"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "12/12 [==============================] - 63s 858ms/step - loss: 0.6575 - accuracy: 0.6347 - val_loss: 0.5395 - val_accuracy: 0.7611 - lr: 2.0000e-05\n",
            "Epoch 2/15\n",
            "12/12 [==============================] - 6s 534ms/step - loss: 0.3973 - accuracy: 0.9000 - val_loss: 0.3032 - val_accuracy: 0.9111 - lr: 2.0000e-05\n",
            "Epoch 3/15\n",
            "12/12 [==============================] - 7s 566ms/step - loss: 0.2237 - accuracy: 0.9431 - val_loss: 0.3152 - val_accuracy: 0.8944 - lr: 2.0000e-05\n",
            "Epoch 4/15\n",
            "12/12 [==============================] - 6s 539ms/step - loss: 0.1476 - accuracy: 0.9653 - val_loss: 0.2274 - val_accuracy: 0.9222 - lr: 2.0000e-05\n",
            "Epoch 5/15\n",
            "12/12 [==============================] - 7s 569ms/step - loss: 0.0720 - accuracy: 0.9889 - val_loss: 0.2371 - val_accuracy: 0.9222 - lr: 2.0000e-05\n",
            "Epoch 6/15\n",
            "12/12 [==============================] - 6s 529ms/step - loss: 0.0500 - accuracy: 0.9931 - val_loss: 0.2462 - val_accuracy: 0.9167 - lr: 2.0000e-05\n",
            "Epoch 7/15\n",
            "12/12 [==============================] - 7s 562ms/step - loss: 0.0279 - accuracy: 0.9972 - val_loss: 0.2476 - val_accuracy: 0.9167 - lr: 4.0000e-06\n",
            "Epoch 8/15\n",
            "12/12 [==============================] - 6s 528ms/step - loss: 0.0216 - accuracy: 1.0000 - val_loss: 0.2569 - val_accuracy: 0.9222 - lr: 4.0000e-06\n",
            "Epoch 9/15\n",
            "12/12 [==============================] - 7s 549ms/step - loss: 0.0194 - accuracy: 1.0000 - val_loss: 0.2581 - val_accuracy: 0.9222 - lr: 4.0000e-06\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f548944c580>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the performance of the BERT model with the LSTM model and comment on your\n",
        "findings.\n",
        "\n",
        "Based on the initial analysis, I found that BERT model is performing much better than the LSTM model, the F1 Score of LSTM model was 0.785, however for BERT it was 0.92."
      ],
      "metadata": {
        "id": "fpIYv2fg8jiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_predictions = bert_model.predict(val_dataset.batch(64))\n",
        "val_pred_labels = np.argmax(val_predictions.logits, axis=1)\n",
        "\n",
        "# Calculate classification report\n",
        "print(classification_report(xtest['label'].values, val_pred_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFXoNPsvajRW",
        "outputId": "ba493657-f0e5-45bc-81b8-35dec3dc1387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 4s 146ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.91      0.92        90\n",
            "           1       0.91      0.93      0.92        90\n",
            "\n",
            "    accuracy                           0.92       180\n",
            "   macro avg       0.92      0.92      0.92       180\n",
            "weighted avg       0.92      0.92      0.92       180\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"F1 Score : {}\".format(f1_score(xtest['label'].values, val_pred_labels)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xAWwpIib6aT",
        "outputId": "582edd33-3a9e-46d1-f548-f8da2a11a4d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score : 0.9230769230769231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply your trained model to the test data (test_yelp_reviews.csv). Save your predictions\n",
        "as a new file in single column titled \"classification\" and export as a CSV file.\n",
        "Please submit a copy of your saved CSV as a separate file on Canvas with the format\n",
        "(<YourNetID>_BERT_predictions.csv). Your grade on the model will be based on the\n",
        "F1-score of your predictions on the unlabeled data."
      ],
      "metadata": {
        "id": "w5tsOrxgBuYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.read_csv('/content/datasets/test_yelp_reviews.csv', sep='\\t',engine='python')\n",
        "\n",
        "# Convert the text data to sequences or embeddings if required by your model\n",
        "test_sequences = tokenizer(list(test_data['text']), truncation=True, padding=True)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_sequences)\n",
        "))\n",
        "\n",
        "# Make predictions\n",
        "test_preds = bert_model.predict(test_dataset.batch(64))\n",
        "# Create a DataFrame with predictions\n",
        "test_preds = np.argmax(test_preds.logits, axis=1)\n",
        "\n",
        "preds_df = pd.DataFrame({'predictions': test_preds})\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "preds_df.to_csv('/content/drive/MyDrive/f006d3c_BERT_predictions.csv', index=False)"
      ],
      "metadata": {
        "id": "I0Tww6xb-hfS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f40187c7-3919-4973-f0ce-283fa643351f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 4s 135ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Large Language Models (30 points)\n",
        "In this part, you will be asked several questions about large language models. No coding is needed.\n",
        "6 points for each question.\n",
        "\n",
        "• Why is it challenging to train language models using reinforcement learning with human feedback (RLHF)? Give three examples to demonstrate.\n",
        "\n",
        "It is challenging to train language models using reinforcement learning with human feedback, as there are predominantly 3 issues that would occur if we were to follow these steps.\n",
        "\n",
        "1. Lack of Scalability as the RLHF-based modeling approach would require a large dataset consisting of human-generated training datasets, this can be time intensive and computationally expensive process.\n",
        "2. Delayed sparse rewards that are commonly received by the language models as feedback signals would make it difficult for the model to learn from its processes to receive rewards making it a slower and unstable process.\n",
        "3. Trade-off between exploration and exploitation: In RLHF, it's critical to strike a balance between the two. Language models must investigate various actions to find superior tactics while making use of previously acquired knowledge. It can be difficult to strike the ideal balance between exploration and exploitation.\n",
        "\n",
        "• What are pros and cons of instruction finetuning and RLHF?\n",
        "\n",
        "Instruction Finetuning is very much interpretable as it makes decision based on the inputs provided by us providing us with precise control on how we would want to see the output of the model.\n",
        "\n",
        "One of the drawbacks for this model is that it is a time consuming and laborious process to provide the model with instructions. It also lacks generalizability and that it would be required to be specifically trained for those scenarios. \n",
        "\n",
        "RLHF on the other hand can be generalizable and would be able to adapt to different scenarios as it learns from feedbacks provided by us. The major drawbacks for RLHF would be that it is very expensive to train the model due to large number of parameters required to train the model.\n",
        "\n",
        "• What are some of the challenges of evaluating the current generation of language models?\n",
        "\n",
        "1. One of the main concerns and issues that plague the current generation of language models is the data quality and diversity of the dataset the model is being trained on. How well the data has been collected affects the model performance, poor quality will often lead the model to overfitting, bias and have generalizibility issues. \n",
        "\n",
        "2. Ethical and social implications is another area of concern that can potentially validate the current generation of language models. These models can impact society, as it can generate misleading or unethical contents. It can also empower people by improving how people communicate with each others, and gain information.\n",
        "\n",
        "3. Selecting adequate evaluation metrics and criteria is the third difficulty that has an impact on language models. The effectiveness and quality of the language models can be measured through evaluation measures. Conventional evaluation metrics, however, might not be adequate or appropriate for all models. For instance, certain metrics may ignore deeper-level factors like coherence, relevance, logic, and creativity in favor of focusing exclusively on the surface-level accuracy or fluency of the created content. Additionally, some criteria could be ambiguous.\n",
        "\n",
        "• Compare three popular language models (you can choose ChatGPT and any other two, e.g., https://www.perplexity.ai/, https://you.com/, https://nat.dev/, https://bard.google.com/). Come up with 5 challenging questions, and compare their output. Comment on the quality of the output and explain what is the possibly the reason.\n",
        "\n",
        "I asked them about various challenging questions focusing on the realm of quantum computing, to applications of Natural language processing in resolving bias and eradicating iliteracy to whether god exists or not. What I found out that the response was similar for the Chatgpt, Perplexity, and you with changing in the way they phrased the sentences. The quality of output was good and it seems that though it seems complicated they were fairly able to avoid any bias or any kind of ethical issues with their responses.\n",
        "\n",
        "\n",
        "\n",
        "Reference : https://www.linkedin.com/advice/1/what-some-current-challenges-limitations-language"
      ],
      "metadata": {
        "id": "0elImAhcCW6K"
      }
    }
  ]
}